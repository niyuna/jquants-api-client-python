#!/usr/bin/env python3
"""
J-Quants API æ•°æ®æ£€æŸ¥å·¥å…·
ç”¨äºè¯»å–å’Œæ£€æŸ¥æœ¬åœ°å­˜å‚¨çš„parquetæ–‡ä»¶
"""

import argparse
import logging
import sys
from pathlib import Path
from datetime import datetime, date
import pandas as pd
import yaml
from typing import Optional, List, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.append(str(Path(__file__).parent.parent))

from scripts.utils.logger import setup_logger

class DataInspector:
    def __init__(self, data_dir: str = "persistdata", config_path: str = "config/api_config.yaml"):
        self.data_dir = Path(data_dir)
        self.config = self._load_config(config_path)
        self.logger = setup_logger()
        
        if not self.data_dir.exists():
            raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            self.logger.warning(f"é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {config_path}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return {}
    
    def list_available_apis(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„APIç›®å½•"""
        api_dirs = []
        for item in self.data_dir.iterdir():
            if item.is_dir() and not item.name.startswith('.'):
                api_dirs.append(item.name)
        return sorted(api_dirs)
    
    def list_available_dates(self, api_name: str) -> List[str]:
        """åˆ—å‡ºæŒ‡å®šAPIçš„å¯ç”¨æ—¥æœŸ"""
        api_dir = self.data_dir / api_name
        if not api_dir.exists():
            return []
        
        dates = []
        for file_path in api_dir.glob("*.parquet"):
            date_str = file_path.stem
            if date_str.isdigit() and len(date_str) == 8:
                dates.append(date_str)
        
        return sorted(dates, reverse=True)
    
    def get_file_info(self, api_name: str, date_str: str) -> Dict[str, Any]:
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        file_path = self.data_dir / api_name / f"{date_str}.parquet"
        
        if not file_path.exists():
            return {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}
        
        try:
            # è·å–æ–‡ä»¶å¤§å°
            file_size = file_path.stat().st_size
            file_size_mb = file_size / (1024 * 1024)
            
            # è¯»å–parquetæ–‡ä»¶å…ƒæ•°æ®ï¼ˆä¸åŠ è½½å…¨éƒ¨æ•°æ®ï¼‰
            parquet_file = pd.read_parquet(file_path, engine='pyarrow')
            
            info = {
                "file_path": str(file_path),
                "file_size_mb": round(file_size_mb, 2),
                "file_size_bytes": file_size,
                "rows": len(parquet_file),
                "columns": list(parquet_file.columns),
                "dtypes": parquet_file.dtypes.to_dict(),
                "memory_usage_mb": round(parquet_file.memory_usage(deep=True).sum() / (1024 * 1024), 2),
                "shape": parquet_file.shape
            }
            
            # æ·»åŠ é…ç½®ä¿¡æ¯
            if api_name in self.config.get('apis', {}):
                api_config = self.config['apis'][api_name]
                info['api_config'] = {
                    'method': api_config.get('method', 'N/A'),
                    'is_range': api_config.get('is_range', False),
                    'plan_required': api_config.get('plan_required', 'N/A')
                }
            
            return info
            
        except Exception as e:
            return {"error": f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}"}
    
    def read_data(self, api_name: str, date_str: str, 
                  head_rows: Optional[int] = None, 
                  tail_rows: Optional[int] = None,
                  sample_rows: Optional[int] = None,
                  columns: Optional[List[str]] = None) -> pd.DataFrame:
        """è¯»å–æ•°æ®"""
        file_path = self.data_dir / api_name / f"{date_str}.parquet"
        
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        try:
            # è¯»å–æ•°æ®
            if columns:
                df = pd.read_parquet(file_path, columns=columns)
            else:
                df = pd.read_parquet(file_path)
            
            # åº”ç”¨è¡Œæ•°é™åˆ¶
            if head_rows is not None:
                df = df.head(head_rows)
            elif tail_rows is not None:
                df = df.tail(tail_rows)
            elif sample_rows is not None:
                df = df.sample(n=min(sample_rows, len(df)), random_state=42)
            
            return df
            
        except Exception as e:
            raise Exception(f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def search_data(self, api_name: str, date_str: str, 
                   search_term: str, search_columns: Optional[List[str]] = None) -> pd.DataFrame:
        """æœç´¢æ•°æ®"""
        df = self.read_data(api_name, date_str)
        
        if search_columns:
            # åœ¨æŒ‡å®šåˆ—ä¸­æœç´¢
            mask = df[search_columns].astype(str).apply(
                lambda x: x.str.contains(search_term, case=False, na=False)
            ).any(axis=1)
        else:
            # åœ¨æ‰€æœ‰åˆ—ä¸­æœç´¢
            mask = df.astype(str).apply(
                lambda x: x.str.contains(search_term, case=False, na=False)
            ).any(axis=1)
        
        return df[mask]
    
    def get_summary_stats(self, api_name: str, date_str: str) -> Dict[str, Any]:
        """è·å–æ•°æ®æ‘˜è¦ç»Ÿè®¡"""
        df = self.read_data(api_name, date_str)
        
        if df.empty:
            return {"error": "æ•°æ®ä¸ºç©º"}
        
        # æ•°å€¼åˆ—ç»Ÿè®¡
        numeric_cols = df.select_dtypes(include=['number']).columns
        numeric_stats = {}
        for col in numeric_cols:
            numeric_stats[col] = {
                'count': df[col].count(),
                'mean': df[col].mean(),
                'std': df[col].std(),
                'min': df[col].min(),
                'max': df[col].max(),
                'null_count': df[col].isnull().sum()
            }
        
        # åˆ†ç±»åˆ—ç»Ÿè®¡
        categorical_cols = df.select_dtypes(include=['object', 'category']).columns
        categorical_stats = {}
        for col in categorical_cols:
            value_counts = df[col].value_counts()
            categorical_stats[col] = {
                'unique_count': df[col].nunique(),
                'null_count': df[col].isnull().sum(),
                'top_values': value_counts.head(5).to_dict()
            }
        
        return {
            'total_rows': len(df),
            'total_columns': len(df.columns),
            'numeric_columns': numeric_stats,
            'categorical_columns': categorical_stats,
            'memory_usage_mb': round(df.memory_usage(deep=True).sum() / (1024 * 1024), 2)
        }

def format_file_info(info: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶ä¿¡æ¯è¾“å‡º"""
    if 'error' in info:
        return f"âŒ {info['error']}"
    
    output = []
    output.append(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {info['file_path']}")
    output.append(f"ğŸ“Š æ–‡ä»¶å¤§å°: {info['file_size_mb']} MB")
    output.append(f"ğŸ“ˆ æ•°æ®è¡Œæ•°: {info['rows']:,}")
    output.append(f"ğŸ”¢ æ•°æ®åˆ—æ•°: {info['shape'][1]}")
    output.append(f"ğŸ’¾ å†…å­˜å ç”¨: {info['memory_usage_mb']} MB")
    
    if 'api_config' in info:
        config = info['api_config']
        output.append(f"ğŸ”§ APIæ–¹æ³•: {config['method']}")
        output.append(f"ğŸ“… æ”¯æŒRange: {'æ˜¯' if config['is_range'] else 'å¦'}")
        output.append(f"ğŸ“‹ è®¡åˆ’è¦æ±‚: {config['plan_required']}")
    
    output.append(f"ğŸ“‹ åˆ—å: {', '.join(info['columns'][:10])}")
    if len(info['columns']) > 10:
        output.append(f"    ... è¿˜æœ‰ {len(info['columns']) - 10} åˆ—")
    
    return '\n'.join(output)

def format_summary_stats(stats: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–æ‘˜è¦ç»Ÿè®¡è¾“å‡º"""
    if 'error' in stats:
        return f"âŒ {stats['error']}"
    
    output = []
    output.append(f"ğŸ“Š æ•°æ®æ‘˜è¦ç»Ÿè®¡")
    output.append(f"æ€»è¡Œæ•°: {stats['total_rows']:,}")
    output.append(f"æ€»åˆ—æ•°: {stats['total_columns']}")
    output.append(f"å†…å­˜å ç”¨: {stats['memory_usage_mb']} MB")
    
    if stats['numeric_columns']:
        output.append(f"\nğŸ”¢ æ•°å€¼åˆ—ç»Ÿè®¡ ({len(stats['numeric_columns'])} åˆ—):")
        for col, col_stats in list(stats['numeric_columns'].items())[:5]:
            output.append(f"  {col}:")
            output.append(f"    å¹³å‡å€¼: {col_stats['mean']:.2f}")
            output.append(f"    æ ‡å‡†å·®: {col_stats['std']:.2f}")
            output.append(f"    èŒƒå›´: [{col_stats['min']:.2f}, {col_stats['max']:.2f}]")
            output.append(f"    ç©ºå€¼: {col_stats['null_count']}")
    
    if stats['categorical_columns']:
        output.append(f"\nğŸ“ åˆ†ç±»åˆ—ç»Ÿè®¡ ({len(stats['categorical_columns'])} åˆ—):")
        for col, col_stats in list(stats['categorical_columns'].items())[:5]:
            output.append(f"  {col}:")
            output.append(f"    å”¯ä¸€å€¼: {col_stats['unique_count']}")
            output.append(f"    ç©ºå€¼: {col_stats['null_count']}")
            output.append(f"    å‰5ä¸ªå€¼: {dict(list(col_stats['top_values'].items())[:5])}")
    
    return '\n'.join(output)

def main():
    parser = argparse.ArgumentParser(
        description='J-Quants API æ•°æ®æ£€æŸ¥å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„API
  python scripts/inspect_data.py --list-apis
  
  # åˆ—å‡ºæŒ‡å®šAPIçš„å¯ç”¨æ—¥æœŸ
  python scripts/inspect_data.py --list-dates daily_quotes
  
  # æŸ¥çœ‹æ–‡ä»¶ä¿¡æ¯
  python scripts/inspect_data.py --info daily_quotes 20240501
  
  # è¯»å–æ•°æ®ï¼ˆå‰10è¡Œï¼‰
  python scripts/inspect_data.py --read daily_quotes 20240501 --head 10
  
  # è¯»å–æ•°æ®ï¼ˆå5è¡Œï¼‰
  python scripts/inspect_data.py --read daily_quotes 20240501 --tail 5
  
  # éšæœºé‡‡æ ·æ•°æ®
  python scripts/inspect_data.py --read daily_quotes 20240501 --sample 20
  
  # æœç´¢æ•°æ®
  python scripts/inspect_data.py --search daily_quotes 20240501 "7203" --columns Code
  
  # è·å–æ‘˜è¦ç»Ÿè®¡
  python scripts/inspect_data.py --stats daily_quotes 20240501
  
  # æŒ‡å®šæ•°æ®ç›®å½•å’Œé…ç½®æ–‡ä»¶
  python scripts/inspect_data.py --data-dir /path/to/data --config /path/to/config.yaml --list-apis
        """
    )
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--data-dir', type=str, default='persistdata',
                       help='æ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: persistdata)')
    parser.add_argument('--config', type=str, default='config/api_config.yaml',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config/api_config.yaml)')
    
    # æ“ä½œæ¨¡å¼
    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument('--list-apis', action='store_true',
                      help='åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„API')
    group.add_argument('--list-dates', type=str, metavar='API_NAME',
                      help='åˆ—å‡ºæŒ‡å®šAPIçš„å¯ç”¨æ—¥æœŸ')
    group.add_argument('--info', nargs=2, metavar=('API_NAME', 'DATE'),
                      help='æ˜¾ç¤ºæŒ‡å®šAPIå’Œæ—¥æœŸçš„æ–‡ä»¶ä¿¡æ¯')
    group.add_argument('--read', nargs=2, metavar=('API_NAME', 'DATE'),
                      help='è¯»å–æŒ‡å®šAPIå’Œæ—¥æœŸçš„æ•°æ®')
    group.add_argument('--search', nargs=3, metavar=('API_NAME', 'DATE', 'SEARCH_TERM'),
                      help='æœç´¢æŒ‡å®šAPIå’Œæ—¥æœŸçš„æ•°æ®')
    group.add_argument('--stats', nargs=2, metavar=('API_NAME', 'DATE'),
                      help='è·å–æŒ‡å®šAPIå’Œæ—¥æœŸçš„æ‘˜è¦ç»Ÿè®¡')
    
    # è¯»å–é€‰é¡¹
    parser.add_argument('--head', type=int, metavar='ROWS',
                       help='è¯»å–å‰Nè¡Œæ•°æ®')
    parser.add_argument('--tail', type=int, metavar='ROWS',
                       help='è¯»å–åNè¡Œæ•°æ®')
    parser.add_argument('--sample', type=int, metavar='ROWS',
                       help='éšæœºé‡‡æ ·Nè¡Œæ•°æ®')
    parser.add_argument('--columns', nargs='+', metavar='COLUMN',
                       help='æŒ‡å®šè¦è¯»å–çš„åˆ—å')
    
    # æœç´¢é€‰é¡¹
    parser.add_argument('--search-columns', nargs='+', metavar='COLUMN',
                       help='æŒ‡å®šè¦æœç´¢çš„åˆ—å')
    
    # è¾“å‡ºé€‰é¡¹
    parser.add_argument('--output', type=str, metavar='FILE',
                       help='å°†ç»“æœè¾“å‡ºåˆ°æ–‡ä»¶ (CSVæ ¼å¼)')
    parser.add_argument('--no-format', action='store_true',
                       help='ç¦ç”¨æ ¼å¼åŒ–è¾“å‡ºï¼Œæ˜¾ç¤ºåŸå§‹æ•°æ®')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºæ£€æŸ¥å™¨
        inspector = DataInspector(args.data_dir, args.config)
        
        # æ‰§è¡Œç›¸åº”æ“ä½œ
        if args.list_apis:
            apis = inspector.list_available_apis()
            print(f"ğŸ“‹ å¯ç”¨çš„API ({len(apis)} ä¸ª):")
            for api in apis:
                print(f"  â€¢ {api}")
        
        elif args.list_dates:
            dates = inspector.list_available_dates(args.list_dates)
            if dates:
                print(f"ğŸ“… {args.list_dates} çš„å¯ç”¨æ—¥æœŸ ({len(dates)} ä¸ª):")
                for date_str in dates[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                    print(f"  â€¢ {date_str}")
                if len(dates) > 20:
                    print(f"  ... è¿˜æœ‰ {len(dates) - 20} ä¸ªæ—¥æœŸ")
            else:
                print(f"âŒ æœªæ‰¾åˆ° {args.list_dates} çš„æ•°æ®")
        
        elif args.info:
            api_name, date_str = args.info
            info = inspector.get_file_info(api_name, date_str)
            print(format_file_info(info))
        
        elif args.read:
            api_name, date_str = args.read
            df = inspector.read_data(
                api_name, date_str,
                head_rows=args.head,
                tail_rows=args.tail,
                sample_rows=args.sample,
                columns=args.columns
            )
            
            if args.output:
                df.to_csv(args.output, index=False, encoding='utf-8')
                print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {args.output}")
            else:
                if args.no_format:
                    print(df.to_string())
                else:
                    print(f"ğŸ“Š {api_name} ({date_str}) æ•°æ®é¢„è§ˆ:")
                    print(f"å½¢çŠ¶: {df.shape}")
                    print(f"åˆ—å: {list(df.columns)}")
                    print("\næ•°æ®é¢„è§ˆ:")
                    print(df.to_string(max_rows=20, max_cols=10))
        
        elif args.search:
            api_name, date_str, search_term = args.search
            df = inspector.search_data(
                api_name, date_str, search_term, args.search_columns
            )
            
            if args.output:
                df.to_csv(args.output, index=False, encoding='utf-8')
                print(f"ğŸ’¾ æœç´¢ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                print(f"ğŸ” åœ¨ {api_name} ({date_str}) ä¸­æœç´¢ '{search_term}' çš„ç»“æœ:")
                print(f"æ‰¾åˆ° {len(df)} è¡Œæ•°æ®")
                if not df.empty:
                    print("\næœç´¢ç»“æœ:")
                    print(df.to_string(max_rows=20, max_cols=10))
        
        elif args.stats:
            api_name, date_str = args.stats
            stats = inspector.get_summary_stats(api_name, date_str)
            print(format_summary_stats(stats))
        
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    main() 